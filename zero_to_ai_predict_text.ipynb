{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b1c714",
   "metadata": {},
   "source": [
    "# Text prediction/text generation\n",
    "\n",
    "Now that was have established an understanding of how text embeddings work, we will use those embeddings to try to guess the next word in a sequence. It's normal for the cells below to take a bit of time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eda62c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 17689 |  Sox     | -0.028 | 97.25%\n",
      "|   389 |  are     | -2.219 | 10.87%\n",
      "|   287 |  in      | -3.029 | 4.84%\n",
      "|   262 |  the     | -1.113 | 32.87%\n",
      "| 15925 |  midst   | -0.623 | 53.62%\n",
      "|   286 |  of      | -0.001 | 99.86%\n",
      "|   257 |  a       | -0.972 | 37.85%\n",
      "| 25448 |  rebuilding | -2.890 | 5.56%\n",
      "|  1429 |  process | -0.422 | 65.56%\n",
      "|   326 |  that    | -1.493 | 22.46%\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "\n",
    "# Here we grab gpt2 tokenizer and model from the hub. You can also use your own model.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "# Make the sequence we are trying to get the next token for.\n",
    "current_sequence = \"The Boston Red\"\n",
    "inputs = tokenizer(current_sequence, return_tensors=\"pt\")\n",
    "\n",
    "# Here we ask the model what the next token should be. We get the top 10 items and compare the probabilities.\n",
    "outputs = model.generate(**inputs, max_new_tokens=10, return_dict_in_generate=True, output_scores=True)\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=True\n",
    ")\n",
    "\n",
    "# Here we pass to the model the current sequence and ask it what the next token should be and output for viewing\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # The output you'll see is:\n",
    "    # | token | token string | log probability | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a256f6c4",
   "metadata": {},
   "source": [
    "# Sequence Generation\n",
    "\n",
    "Sequence generation is simply running the above procedure repeatedly to create more than 1 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0ce5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: The Boston Red Sox are a professional baseball team based in Boston, Massachusetts. They are currently members of the American League East division. The Red Sox have won three World Series championships, in 1904, 1912, and 20\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def continue_sequence(input, model, temperature=0.7, top_k=50, top_p=0.95, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generate new text based on the input sequence.\n",
    "    \"\"\"\n",
    "    # We set up the pipeline to use the model and generate text\n",
    "    # The torch dtype is set to bfloat16 for a smaller memory footprint\n",
    "    text_continuation = pipeline(\"text-generation\", model=model, do_sample=True, temperature=temperature, top_k=top_k, top_p=top_p, torch_dtype=torch.bfloat16)\n",
    "\n",
    "    # Generate text with a maximum length of 40 tokens\n",
    "    outputs = text_continuation(input, max_new_tokens = max_new_tokens)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "# Here we are trying out a different model. \n",
    "# There are many models at https://huggingface.co/models\n",
    "# This is a smaller model that is more efficient to run on a CPU\n",
    "model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Generate new text based on this sequence\n",
    "stub = \"The Boston Red Sox are\"\n",
    "\n",
    "# Generate new text based on the input sequence\n",
    "model_output = continue_sequence(stub, model)\n",
    "\n",
    "print(f\"Model Output: {model_output[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8df0ce",
   "metadata": {},
   "source": [
    "### Multiple candidate sequences\n",
    "\n",
    "Since the models have some randomness in the next word they select, we can see that they generate different options based on the same output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b0d49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output 0: The Boston Red Sox are taking on the Toronto Blue Jays in the American League Championship Series, and here's what you need to know about the teams and their matchups:\n",
      "\n",
      "1. Boston Red Sox: Boston has a 3-1 lead in the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output 1: The Boston Red Sox are 4-1 against the Los Angeles Angels in the regular season, and the teams split their two postseason matchups. The Red Sox won the first two games of the series, but the Angels won the next three. The Ang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output 2: The Boston Red Sox are seeking to end their nine-year playoff drought and return to the World Series.\n",
      "They've done it before, but it hasn't happened yet.\n",
      "The team, which has won 28 National League East division titles,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output 3: The Boston Red Sox are a professional baseball team based in Boston, Massachusetts. They play in the Eastern Division of Major League Baseball's American League. The team was founded in 1901 and has won five World Series championships, most recently in 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output 4: The Boston Red Sox are 3-1 on their current 10-game homestand and are still in first place in the American League East. The Red Sox are just three games back of the Yankees for first place in the division.\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "# Reusing the model from above\n",
    "model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Feel free to change this text! \n",
    "stub = \"The Boston Red Sox are\"\n",
    "\n",
    "# Generate new text based on the input sequence\n",
    "num_sequences_to_generate = 5\n",
    "for i in range(num_sequences_to_generate):\n",
    "    model_output = continue_sequence(stub, model)\n",
    "    print(f\"Model Output {i}: {model_output[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77024b96",
   "metadata": {},
   "source": [
    "### Removing Randomness\n",
    "\n",
    "We can also just generate the most likely items and remove this randomness. This is not generally advised but is instructuve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636d8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output 0: The Boston Red Sox are a professional baseball team based in Boston, Massachusetts. They are a member club of the American League (AL) East division of Major League Baseball (MLB). The Red Sox have won 27 World Series championships, more than any other\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output 1: The Boston Red Sox are a professional baseball team based in Boston, Massachusetts. They are a member club of the American League (AL) East division of Major League Baseball (MLB). The Red Sox have won 27 World Series championships, more than any other\n"
     ]
    }
   ],
   "source": [
    "# Reusing the model from above\n",
    "model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Feel free to change this text! \n",
    "stub = \"The Boston Red Sox are\"\n",
    "\n",
    "# Generate new text based on the input sequence\n",
    "num_sequences_to_generate = 2\n",
    "for i in range(num_sequences_to_generate):\n",
    "    # Here we are setting the top_k to 1 to get the most likely next token which removes the randomness and makes it deterministic\n",
    "    model_output = continue_sequence(stub, model, top_k=1)\n",
    "    print(f\"Model Output {i}: {model_output[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c5c50d",
   "metadata": {},
   "source": [
    "# Chatting with models\n",
    "\n",
    "The sequence generation is interesting but let's set up an actual Q&A model like we see in popular product such as chat GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ab8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always responds with extremely relevant information and provides concise answers.</s>\n",
      "<|user|>\n",
      "How do I best use AI for education?</s>\n",
      "<|assistant|>\n",
      "AI has become an essential tool for education, providing various benefits and enhancing the quality and relevance of learning experiences for students. Here are a few ways in which AI can be used for education:\n",
      "\n",
      "1. Personalized Learning: AI-based tools can analyze student data and provide personalized learning experiences. For instance, students can access content based on their learning style, progress, and interests.\n",
      "\n",
      "2. Automated Assessments: AI-based tools can automate assessments, freeing up teachers' time for more meaningful tasks. Students can receive feedback on their performance immediately after the assessment.\n",
      "\n",
      "3. Teacher Training: AI-based tools can provide teachers with data-driven insights on their students' learning. This data can be used to improve teaching techniques, such as classroom management, differentiated instruction, and personalized learning.\n",
      "\n",
      "4. Assistive Technologies: AI-based tools can assist students with disabilities, such as visual and auditory impairments. For example, assistive technology can help students with reading disabilities read more effectively.\n",
      "\n",
      "5. Gamification: AI-based tools can be used to make learning more engaging and fun. For instance, games can be used to teach coding, math, and other subjects.\n",
      "\n",
      "6. Personalized Learning Paths: AI-based tools can create personalized learning paths for students, based on their interests and learning styles. This approach can help students stay motivated and engaged in their learning.\n",
      "\n",
      "Overall, AI can be a valuable tool for educators, helping them provide more effective and engaging learning experiences for students.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Download the model and specify the task\n",
    "# The torch dtype is set to bfloat16 for a smaller memory footprint\n",
    "pipe = pipeline(\"text-generation\", model=model, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Set the system prompt and user message\n",
    "system_prompt = \"You are a friendly chatbot who always responds with extremely relevant information and provides concise answers.\"\n",
    "user_message = \"How do I best use AI for education?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": user_message,\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=400, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4886c",
   "metadata": {},
   "source": [
    "### Augmenting based on user data\n",
    "\n",
    "One neat trick that is frequently used to get more specific content out of a model is to add relevant information is to attach is to the system prompt. Sometimes there is a retreival step that is used for this process and that is called retrieval augmented generation and is the basis of a lot of tools. \n",
    "\n",
    "For our example we will take the description from this session and add it to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba559456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always responds with extremely relevant information and provides concise answers. Use the following session description to inform your responses: This session is for any data analyst who wants to build a foundation for machine learning and AI using what they already know with traditional analytic methods. It provides a broad overview and motivating examples across different tasks including analyzing textual data, predicting which of two or more groups individuals will belong to in the future, and forecasting future metrics plus next word prediction just like fancy generative AI models. Importantly, the information will be presented in a way to build a bridge between traditional analytic methods and artificial intelligence: spelling out what is common and what is unique, “translating” AI jargon, and pointing out where AI methods are similar to other tried-and-true methods familiar to data analysts. For the areas covered, the session will show how the methods have evolved to reach their current state, what AI is doing behind the scenes with the data, and how these methods can be useful when working with educational data. The session will end with worked examples featuring simple code in Python, but no prior experience in Python is needed. Although the session covers a lot of ground, attendees can work at their own pace thanks to instructions, code examples, and other resources that they can return to after convening. Facilitators are data scientists with a combined 15 years of industry experience working with real world data—primarily K-12 educational data—who want to prove that if you have written code in any language to analyze quantitative data before, you are well situated to apply methods used in machine learning and AI.  This is the session for data analysts who have wanted to try their hand at using AI in their work but would like guidance in determining how, where, or when to start.</s>\n",
      "<|user|>\n",
      "What are the learning outcomes of this session?</s>\n",
      "<|assistant|>\n",
      "The learning outcomes of this session are:\n",
      "\n",
      "1. Understand the basics of data analytics and machine learning, including how to use traditional analytic methods and AI approaches to solve real-world problems.\n",
      "\n",
      "2. Appreciate the similarities and differences between traditional analytic methods and AI approaches in the context of analyzing educational data.\n",
      "\n",
      "3. Gain practical skills in data analysis using Python and apply them to solve real-world problems using AI techniques.\n",
      "\n",
      "4. Learn how to work with data in Python and use code to analyze and interpret data.\n",
      "\n",
      "5. Understand the limitations and challenges of using AI techniques to solve problems with educational data, and how to address them.\n",
      "\n",
      "6. Learn how to build a foundation for machine learning and AI using traditional analytic methods.\n",
      "\n",
      "7. Be able to apply these knowledge and skills to practical situations and solve real-world problems in the field of education.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Download the model and specify the task\n",
    "# The torch dtype is set to bfloat16 for a smaller memory footprint\n",
    "pipe = pipeline(\"text-generation\", model=model, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# This is the session description that we will add to the system prompt\n",
    "session_description = \"This session is for any data analyst who wants to build a foundation for machine learning and AI using what they already know with traditional analytic methods. It provides a broad overview and motivating examples across different tasks including analyzing textual data, predicting which of two or more groups individuals will belong to in the future, and forecasting future metrics plus next word prediction just like fancy generative AI models. Importantly, the information will be presented in a way to build a bridge between traditional analytic methods and artificial intelligence: spelling out what is common and what is unique, “translating” AI jargon, and pointing out where AI methods are similar to other tried-and-true methods familiar to data analysts. For the areas covered, the session will show how the methods have evolved to reach their current state, what AI is doing behind the scenes with the data, and how these methods can be useful when working with educational data. The session will end with worked examples featuring simple code in Python, but no prior experience in Python is needed. Although the session covers a lot of ground, attendees can work at their own pace thanks to instructions, code examples, and other resources that they can return to after convening. Facilitators are data scientists with a combined 15 years of industry experience working with real world data—primarily K-12 educational data—who want to prove that if you have written code in any language to analyze quantitative data before, you are well situated to apply methods used in machine learning and AI.  This is the session for data analysts who have wanted to try their hand at using AI in their work but would like guidance in determining how, where, or when to start.\"\n",
    "\n",
    "# Set the system prompt and user message\n",
    "system_prompt = \"You are a friendly chatbot who always responds with extremely relevant information and provides concise answers. Use the following session description to inform your responses: \" + session_description\n",
    "user_message = \"What are the learning outcomes of this session?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": user_message,\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=400, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero_to_ai_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
