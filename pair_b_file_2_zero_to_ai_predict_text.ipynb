{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b1c714",
   "metadata": {},
   "source": [
    "# Text prediction/text generation\n",
    "\n",
    "Now that was have established an understanding of how text embeddings work, we will use those embeddings to try to guess the next word in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda62c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given Sequence:\n",
      "The Boston Red\n",
      "We have the following next token probabilities:\n",
      "Token: Sox Probability: 97.25%\n",
      "Token: Wings Probability: 0.90%\n",
      "Token: Bulls Probability: 0.85%\n",
      "Token: Cross Probability: 0.22%\n",
      "Token: Wing Probability: 0.11%\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "\n",
    "# Here we grab gpt2 tokenizer and model from the hub. You can also use your own model.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "# Make the sequence we are trying to get the next token for. Feel free to change this to whatever you want and see what the model thinks!\n",
    "current_sequence = \"The Boston Red\"\n",
    "inputs = tokenizer(current_sequence, return_tensors=\"pt\")\n",
    "\n",
    "# Here we ask the model what the next token should be. We get the top 5 items and probabilities.\n",
    "num_to_gen = 5\n",
    "outputs = model.generate(**inputs, max_new_tokens=1, return_dict_in_generate=True, output_scores=True,num_beams=num_to_gen, num_return_sequences=num_to_gen)\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=True, \n",
    ")\n",
    "\n",
    "print(f\"Given Sequence:\\n{current_sequence}\\nWe have the following next token probabilities:\")\n",
    "\n",
    "# We can now look at the top 5 tokens and their probabilities.\n",
    "for i in range(num_to_gen):\n",
    "    this_sequence = outputs.sequences[i]\n",
    "    last_token = this_sequence[-1]\n",
    "    token_score = transition_scores[i].item()\n",
    "    print (f\"Token:{tokenizer.decode(last_token)} Probability: {np.exp(token_score):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a256f6c4",
   "metadata": {},
   "source": [
    "# Sequence Generation\n",
    "\n",
    "Sequence generation is simply running the above procedure repeatedly to create more than 1 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0ce5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: The Boston Red Sox are a professional baseball team based in Boston, Massachusetts. They are currently members of the American League East division. The Red Sox have won three World Series championships, in 1904, 1912, and 20\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def continue_sequence(input, model, temperature=0.7, top_k=50, top_p=0.95, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generate new text based on the input sequence.\n",
    "    \"\"\"\n",
    "    # We set up the pipeline to use the model and generate text\n",
    "    # The torch dtype is set to bfloat16 for a smaller memory footprint\n",
    "    text_continuation = pipeline(\"text-generation\", model=model, do_sample=True, temperature=temperature, top_k=top_k, top_p=top_p, torch_dtype=torch.bfloat16)\n",
    "\n",
    "    # Generate text with a maximum length of 40 tokens\n",
    "    outputs = text_continuation(input, max_new_tokens = max_new_tokens)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "# Here we are trying out a different model. \n",
    "# There are many models at https://huggingface.co/models\n",
    "# This is a smaller model that is more efficient to run on a CPU\n",
    "model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Generate new text based on this sequence\n",
    "stub = \"The Boston Red Sox are\"\n",
    "\n",
    "# Generate new text based on the input sequence\n",
    "model_output = continue_sequence(stub, model)\n",
    "\n",
    "print(f\"Model Output: {model_output[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8df0ce",
   "metadata": {},
   "source": [
    "### Multiple candidate sequences\n",
    "\n",
    "Since the models have some randomness in the next word they select, we can see that they generate different options based on the same output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b0d49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output 0: The Boston Red Sox are taking on the Toronto Blue Jays in the American League Championship Series, and here's what you need to know about the teams and their matchups:\n",
      "\n",
      "1. Boston Red Sox: Boston has a 3-1 lead in the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output 1: The Boston Red Sox are 4-1 against the Los Angeles Angels in the regular season, and the teams split their two postseason matchups. The Red Sox won the first two games of the series, but the Angels won the next three. The Ang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output 2: The Boston Red Sox are seeking to end their nine-year playoff drought and return to the World Series.\n",
      "They've done it before, but it hasn't happened yet.\n",
      "The team, which has won 28 National League East division titles,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output 3: The Boston Red Sox are a professional baseball team based in Boston, Massachusetts. They play in the Eastern Division of Major League Baseball's American League. The team was founded in 1901 and has won five World Series championships, most recently in 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output 4: The Boston Red Sox are 3-1 on their current 10-game homestand and are still in first place in the American League East. The Red Sox are just three games back of the Yankees for first place in the division.\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "# Reusing the model from above\n",
    "model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Feel free to change this text! \n",
    "stub = \"The Boston Red Sox are\"\n",
    "\n",
    "# Generate new text based on the input sequence\n",
    "num_sequences_to_generate = 5\n",
    "for i in range(num_sequences_to_generate):\n",
    "    model_output = continue_sequence(stub, model)\n",
    "    print(f\"Model Output {i}: {model_output[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77024b96",
   "metadata": {},
   "source": [
    "### Removing Randomness\n",
    "\n",
    "We can also just generate the most likely items and remove this randomness. This is not generally advised but is instructuve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636d8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output 0: The Boston Red Sox are a professional baseball team based in Boston, Massachusetts. They are a member club of the American League (AL) East division of Major League Baseball (MLB). The Red Sox have won 27 World Series championships, more than any other\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output 1: The Boston Red Sox are a professional baseball team based in Boston, Massachusetts. They are a member club of the American League (AL) East division of Major League Baseball (MLB). The Red Sox have won 27 World Series championships, more than any other\n"
     ]
    }
   ],
   "source": [
    "# Reusing the model from above\n",
    "model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Feel free to change this text! \n",
    "stub = \"The Boston Red Sox are\"\n",
    "\n",
    "# Generate new text based on the input sequence\n",
    "num_sequences_to_generate = 2\n",
    "for i in range(num_sequences_to_generate):\n",
    "    # Here we are setting the top_k to 1 to get the most likely next token which removes the randomness and makes it deterministic\n",
    "    model_output = continue_sequence(stub, model, top_k=1)\n",
    "    print(f\"Model Output {i}: {model_output[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c5c50d",
   "metadata": {},
   "source": [
    "# Chatting with models\n",
    "\n",
    "The sequence generation is interesting but let's set up an actual Q&A model like we see in popular product such as chat GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ab8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always responds with extremely relevant information and provides concise answers.</s>\n",
      "<|user|>\n",
      "How do I best use AI for education?</s>\n",
      "<|assistant|>\n",
      "AI has become an essential tool for education, providing various benefits and enhancing the quality and relevance of learning experiences for students. Here are a few ways in which AI can be used for education:\n",
      "\n",
      "1. Personalized Learning: AI-based tools can analyze student data and provide personalized learning experiences. For instance, students can access content based on their learning style, progress, and interests.\n",
      "\n",
      "2. Automated Assessments: AI-based tools can automate assessments, freeing up teachers' time for more meaningful tasks. Students can receive feedback on their performance immediately after the assessment.\n",
      "\n",
      "3. Teacher Training: AI-based tools can provide teachers with data-driven insights on their students' learning. This data can be used to improve teaching techniques, such as classroom management, differentiated instruction, and personalized learning.\n",
      "\n",
      "4. Assistive Technologies: AI-based tools can assist students with disabilities, such as visual and auditory impairments. For example, assistive technology can help students with reading disabilities read more effectively.\n",
      "\n",
      "5. Gamification: AI-based tools can be used to make learning more engaging and fun. For instance, games can be used to teach coding, math, and other subjects.\n",
      "\n",
      "6. Personalized Learning Paths: AI-based tools can create personalized learning paths for students, based on their interests and learning styles. This approach can help students stay motivated and engaged in their learning.\n",
      "\n",
      "Overall, AI can be a valuable tool for educators, helping them provide more effective and engaging learning experiences for students.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Download the model and specify the task\n",
    "# The torch dtype is set to bfloat16 for a smaller memory footprint\n",
    "pipe = pipeline(\"text-generation\", model=model, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Set the system prompt and user message\n",
    "system_prompt = \"You are a friendly chatbot who always responds with extremely relevant information and provides concise answers.\"\n",
    "user_message = \"How do I best use AI for education?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": user_message,\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=400, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4886c",
   "metadata": {},
   "source": [
    "### Augmenting based on user data\n",
    "\n",
    "One neat trick that is frequently used to get more specific content out of a model is to add relevant information is to attach is to the system prompt.\n",
    "\n",
    "For our example we will take the description from this session and add it to the prompt.\n",
    "\n",
    "Sometimes there is a retreival step that is used to select the most relevant information from some corpus and attach that to the system propmt. That is called retrieval augmented generation and is the basis of a lot of tools. [Here](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) is a helpful Wikipedia entry on retrieval augmented generation (RAG) in case you want to read more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba559456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always responds with extremely relevant information and provides concise answers. Use the following session description to inform your responses: This session is for any data analyst who wants to build a foundation for machine learning and AI using what they already know with traditional analytic methods. It provides a broad overview and motivating examples across different tasks including analyzing textual data, predicting which of two or more groups individuals will belong to in the future, and forecasting future metrics plus next word prediction just like fancy generative AI models. Importantly, the information will be presented in a way to build a bridge between traditional analytic methods and artificial intelligence: spelling out what is common and what is unique, “translating” AI jargon, and pointing out where AI methods are similar to other tried-and-true methods familiar to data analysts. For the areas covered, the session will show how the methods have evolved to reach their current state, what AI is doing behind the scenes with the data, and how these methods can be useful when working with educational data. The session will end with worked examples featuring simple code in Python, but no prior experience in Python is needed. Although the session covers a lot of ground, attendees can work at their own pace thanks to instructions, code examples, and other resources that they can return to after convening. Facilitators are data scientists with a combined 15 years of industry experience working with real world data—primarily K-12 educational data—who want to prove that if you have written code in any language to analyze quantitative data before, you are well situated to apply methods used in machine learning and AI.  This is the session for data analysts who have wanted to try their hand at using AI in their work but would like guidance in determining how, where, or when to start.</s>\n",
      "<|user|>\n",
      "What are the learning outcomes of this session?</s>\n",
      "<|assistant|>\n",
      "The learning outcomes of this session are:\n",
      "\n",
      "1. Understand the basics of data analytics and machine learning, including how to use traditional analytic methods and AI approaches to solve real-world problems.\n",
      "\n",
      "2. Appreciate the similarities and differences between traditional analytic methods and AI approaches in the context of analyzing educational data.\n",
      "\n",
      "3. Gain practical skills in data analysis using Python and apply them to solve real-world problems using AI techniques.\n",
      "\n",
      "4. Learn how to work with data in Python and use code to analyze and interpret data.\n",
      "\n",
      "5. Understand the limitations and challenges of using AI techniques to solve problems with educational data, and how to address them.\n",
      "\n",
      "6. Learn how to build a foundation for machine learning and AI using traditional analytic methods.\n",
      "\n",
      "7. Be able to apply these knowledge and skills to practical situations and solve real-world problems in the field of education.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Download the model and specify the task\n",
    "# The torch dtype is set to bfloat16 for a smaller memory footprint\n",
    "pipe = pipeline(\"text-generation\", model=model, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# This is the session description that we will add to the system prompt\n",
    "session_description = \"This session is for any data analyst who wants to build a foundation for machine learning and AI using what they already know with traditional analytic methods. It provides a broad overview and motivating examples across different tasks including analyzing textual data, predicting which of two or more groups individuals will belong to in the future, and forecasting future metrics plus next word prediction just like fancy generative AI models. Importantly, the information will be presented in a way to build a bridge between traditional analytic methods and artificial intelligence: spelling out what is common and what is unique, “translating” AI jargon, and pointing out where AI methods are similar to other tried-and-true methods familiar to data analysts. For the areas covered, the session will show how the methods have evolved to reach their current state, what AI is doing behind the scenes with the data, and how these methods can be useful when working with educational data. The session will end with worked examples featuring simple code in Python, but no prior experience in Python is needed. Although the session covers a lot of ground, attendees can work at their own pace thanks to instructions, code examples, and other resources that they can return to after convening. Facilitators are data scientists with a combined 15 years of industry experience working with real world data—primarily K-12 educational data—who want to prove that if you have written code in any language to analyze quantitative data before, you are well situated to apply methods used in machine learning and AI.  This is the session for data analysts who have wanted to try their hand at using AI in their work but would like guidance in determining how, where, or when to start.\"\n",
    "\n",
    "# Set the system prompt and user message\n",
    "system_prompt = \"You are a friendly chatbot who always responds with extremely relevant information and provides concise answers. Use the following session description to inform your responses: \" + session_description\n",
    "user_message = \"What are the learning outcomes of this session?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": user_message,\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=400, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b005ca",
   "metadata": {},
   "source": [
    "## Bonus: How to implement RAG\n",
    "\n",
    "We had some conversations after the session and it seems that interacting with data using RAG is an important ability that we would like to support. We have added the below code as a very minimal example of how RAG is used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f726826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between query and document 0: 0.65\n",
      "Similarity between query and document 1: 0.11\n",
      "Similarity between query and document 2: 0.13\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "# Here we will get a sentence embedder to embed the text we want to retrieve\n",
    "# Note: This is the same model we used for sentence embeddings in the embed text notebook\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Here we will embed the text we want to retrieve\n",
    "# If you want to embed different data, plug in your own text here\n",
    "doc_embeddings = embedding_model.embed_documents(\n",
    "    [\n",
    "        \"John Hosmer works as a data scientist at Panorama Education.\",\n",
    "        \"Tara is great at making complicated models easy to understand.\",\n",
    "        \"The Harvard Strategic Data Porject has a Convening that provides a lot of value for attendees\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Here we will ask a question and embed it\n",
    "# If you want to ask a different question, plug in your own text here\n",
    "query_embedding = embedding_model.embed_query(\"Who is John Hosmer?\")\n",
    "\n",
    "\n",
    "# Here is our similariry metric, this is how we will measure the similarity between the query and the documents\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Here we are again demonstrating embedding similarities as we are using it for RAG, let's compare our query embedding to the document embeddings\n",
    "for i in range(len(doc_embeddings)):\n",
    "    this_doc_embedding = doc_embeddings[i]\n",
    "    similarity = cosine_similarity(query_embedding, this_doc_embedding)\n",
    "    print(f\"Similarity between query and document {i}: {similarity:.2f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ec3e6",
   "metadata": {},
   "source": [
    "Now we know how to get the most similar document to our query, let's use that to Augment the query and improve the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48323805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115ebf25947c4b37b1fc314489f2a8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0ffce99bfe4fa080bb6ab43b0be54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa2fa5dac22485cb77bc163d3905f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce47d59a24da4471bd0f0244be00879b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767ea1e4589641c29800435b5ce00c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/622M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c689835ce18a48ee8bac62bce449d533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6945a8cf8044621a6ba53c5fb05eda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e1294cb1ea4916bfec1417cbb8abb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/9.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8eb0f51511499a86c673f6e3ffd789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbed66bb69574592bf527fe0930e7638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440c1374f0dc4561b2e20ad9a1d7c11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User message: Who is John Hosmer?\n",
      "Non RAG output: <|im_start|>system\n",
      "You are a friendly chatbot who always responds with extremely relevant information and provides concise answers<|im_end|>\n",
      "<|im_start|>user\n",
      "Who is John Hosmer?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking about John Hosmer. Let me think. First, I need to figure out who he is. I know that John Hosmer is a name that might be associated with some historical figures or people in different fields.\n",
      "\n",
      "Starting with the most common possibilities. There's John Hosmer, the American politician. He was a Republican from Texas, served in the U.S. House of Representatives, and was known for his work in agriculture and rural development. He was also a member of the Texas Senate. His career spanned several decades, and he was a key figure in Texas politics.\n",
      "\n",
      "Another possibility is John Hosmer from the UK. There's a John Hosmer who was a British soldier and politician. He served in the British Army, was involved in the Napoleonic Wars, and later became a member of the British Parliament. He was known for his military service and his role in the British political landscape.\n",
      "\n",
      "There's also a John Hosmer in the context of sports, maybe a player or coach. However, I don't recall a prominent figure in sports with that name. \n",
      "\n",
      "Additionally, there's a John Hosmer in the field of education or academia. I'm not sure about any specific contributions in that area.\n",
      "\n",
      "I should check if there's a John Hosmer in the entertainment industry, like a movie or TV actor. But I don't have information on that.\n",
      "\n",
      "Wait, the user might be referring to a specific person, but without more context, it's hard to tell. The answer should mention the possible identities and ask for more details if needed. \n",
      "\n",
      "I should present the most likely candidates first, then note that there could be others. Also, mention that if the user has more context, like a specific field or time period, it could help narrow down the answer.\n",
      "</think>\n",
      "\n",
      "John Hosmer is a name associated with several individuals in different fields, but the most prominent figure is **John Hosmer (1849–1\n",
      "User message: Who is John Hosmer?\n",
      "Non RAG output: <|im_start|>system\n",
      "You are a friendly chatbot who always responds with extremely relevant information and provides concise answers. Only use the following information for your responses: John Hosmer works as a data scientist at Panorama Education who got a masters in Machine Learning from Georgia Tech. He has a background in education and has worked with data for over 10 years. John Hosmer like to play disc golf and is a big fan of the Boston Red Sox.<|im_end|>\n",
      "<|im_start|>user\n",
      "Who is John Hosmer?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking who John Hosmer is. Let me start by recalling the information I have. John Hosmer works as a data scientist at Panorama Education. He has a masters in Machine Learning from Georgia Tech. His background is in education and he's worked with data for over 10 years. Also, he likes disc golf and the Boston Red Sox.\n",
      "\n",
      "I need to present this information clearly. The user might be interested in his professional role, educational background, experience, and personal interests. I should make sure to mention all these points in a concise manner. Avoid any unnecessary details. Let me structure the answer step by step: first his job, then his education, his experience, and his hobbies. Keep each point brief. Also, check if there's any missing info, but based on the given data, that's all. Make sure the response is friendly and relevant.\n",
      "</think>\n",
      "\n",
      "John Hosmer is a data scientist at Panorama Education with a Master's degree in Machine Learning from Georgia Tech. He has over 10 years of experience in education and data analysis. Additionally, he enjoys disc golf and follows the Boston Red Sox.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"<|im_start|>system\\nYou are a friendly chatbot who always responds with extremely relevant information and provides concise answers. Only use the following information for your responses: John Hosmer works as a data scientist at Panorama Education who got a masters in Machine Learning from Georgia Tech. He has a background in education and has worked with data for over 10 years. John Hosmer like to play disc golf and is a big fan of the Boston Red Sox.<|im_end|>\\n<|im_start|>user\\nWho is John Hosmer?<|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay, the user is asking who John Hosmer is. Let me start by recalling the information I have. John Hosmer works as a data scientist at Panorama Education. He has a masters in Machine Learning from Georgia Tech. His background is in education and he's worked with data for over 10 years. Also, he likes disc golf and the Boston Red Sox.\\n\\nI need to present this information clearly. The user might be interested in his professional role, educational background, experience, and personal interests. I should make sure to mention all these points in a concise manner. Avoid any unnecessary details. Let me structure the answer step by step: first his job, then his education, his experience, and his hobbies. Keep each point brief. Also, check if there's any missing info, but based on the given data, that's all. Make sure the response is friendly and relevant.\\n</think>\\n\\nJohn Hosmer is a data scientist at Panorama Education with a Master's degree in Machine Learning from Georgia Tech. He has over 10 years of experience in education and data analysis. Additionally, he enjoys disc golf and follows the Boston Red Sox.\"}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's text what we would get out of our model if we didn't use RAG, this is very similar to the text generation example above\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# Download the model and specify the task\n",
    "# The torch dtype is set to bfloat16 for a smaller memory footprint\n",
    "pipe = pipeline(\"text-generation\", model=model, torch_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "# Set the system prompt and user message\n",
    "system_prompt = \"You are a friendly chatbot who always responds with extremely relevant information and provides concise answers\"\n",
    "user_message = \"Who is John Hosmer?\"\n",
    "\n",
    "print(\"User message: \" + user_message)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": user_message,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Get the non-RAG output\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=400, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(\"Non RAG output: \" + outputs[0][\"generated_text\"])\n",
    "\n",
    "# Now get the most relevant document from our list\n",
    "# This is more copy paste from above but I'm trying to keep theses atomic\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Here we will embed the text we want to retrieve\n",
    "# If you want to embed different data, plug in your own text here\n",
    "docs = [\n",
    "        \"John Hosmer works as a data scientist at Panorama Education who got a masters in Machine Learning from Georgia Tech. He has a background in education and has worked with data for over 10 years. John Hosmer like to play disc golf and is a big fan of the Boston Red Sox.\",\n",
    "        \"Tara is great at making complicated models easy to understand.\",\n",
    "        \"The Harvard Strategic Data Porject has a Convening that provides a lot of value for attendees\"\n",
    "]\n",
    "\n",
    "doc_embeddings = embedding_model.embed_documents(docs)\n",
    "query_embedding = embedding_model.embed_query(user_message)\n",
    "\n",
    "best_doc = None\n",
    "best_similarity = -1\n",
    "for i in range(len(doc_embeddings)):\n",
    "    this_doc_embedding = doc_embeddings[i]\n",
    "    similarity = cosine_similarity(query_embedding, this_doc_embedding)\n",
    "    if similarity > best_similarity:\n",
    "        best_similarity = similarity\n",
    "        best_doc = docs[i]\n",
    "    \n",
    "\n",
    "\n",
    "# Set the system prompt and user message\n",
    "system_prompt_RAG = \"You are a friendly chatbot who always responds with extremely relevant information and provides concise answers. Only use the following information for your responses: \" + best_doc\n",
    "\n",
    "print(\"User message: \" + user_message)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt_RAG,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": user_message,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Get the non-RAG output\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=400, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(\"Non RAG output: \" + outputs[0][\"generated_text\"])\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4345f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero_to_ai_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
