{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf8d7a8",
   "metadata": {},
   "source": [
    "# NLP: Embeddings\n",
    "\n",
    "Welcome to the embedding portion of this session!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68e8d2",
   "metadata": {},
   "source": [
    "## Part 1: word2vec word embeddings\n",
    "\n",
    "We will start off by using word2vec to take words and embed them into a vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9187aeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25233    0.10176   -0.67485    0.21117    0.43492    0.16542\n",
      "  0.48261   -0.81222    0.041321   0.78502   -0.077857  -0.66324\n",
      "  0.1464    -0.29289   -0.25488    0.019293  -0.20265    0.98232\n",
      "  0.028312  -0.081276  -0.1214     0.13126   -0.17648    0.13556\n",
      " -0.16361   -0.22574    0.055006  -0.20308    0.20718    0.095785\n",
      "  0.22481    0.21537   -0.32982   -0.12241   -0.40031   -0.079381\n",
      " -0.19958   -0.015083  -0.079139  -0.18132    0.20681   -0.36196\n",
      " -0.30744   -0.24422   -0.23113    0.09798    0.1463    -0.062738\n",
      "  0.42934   -0.078038  -0.19627    0.65093   -0.22807   -0.30308\n",
      " -0.12483   -0.17568   -0.14651    0.15361   -0.29518    0.15099\n",
      " -0.51726   -0.033564  -0.23109   -0.7833     0.018029  -0.15719\n",
      "  0.02293    0.49639    0.029225   0.05669    0.14616   -0.19195\n",
      "  0.16244    0.23898    0.36431    0.45263    0.2456     0.23803\n",
      "  0.31399    0.3487    -0.035791   0.56108   -0.25345    0.051964\n",
      " -0.10618   -0.30962    1.0585    -0.42025    0.18216   -0.11256\n",
      "  0.40576    0.11784   -0.19705   -0.075292   0.080723  -0.02782\n",
      " -0.15617   -0.44681   -0.15165    0.1692     0.098255  -0.031894\n",
      "  0.087143   0.26082    0.002706   0.1319     0.34439   -0.37894\n",
      " -0.4114     0.081571  -0.11674   -0.43711    0.011144   0.099353\n",
      "  0.26612    0.40025    0.18895   -0.18438   -0.30355   -0.2725\n",
      "  0.22468   -0.40614    0.15618   -0.16043    0.47147    0.0080203\n",
      "  0.56858    0.21934   -0.11181    0.79925    0.10714   -0.50146\n",
      "  0.063593   0.069465   0.15292   -0.2747    -0.20989    0.20737\n",
      " -0.10681    0.40651   -2.6438    -0.31139   -0.32157   -0.26458\n",
      " -0.35625    0.070013  -0.18838    0.48773   -0.26167   -0.020805\n",
      "  0.17819    0.15758   -0.13752    0.056464   0.30766   -0.066136\n",
      "  0.4748    -0.27335    0.09732   -0.20832    0.0039332  0.346\n",
      " -0.08702   -0.54924   -0.18759   -0.17174    0.060324  -0.13521\n",
      "  0.10419    0.30165    0.05798    0.21872   -0.073594  -0.20423\n",
      " -0.25279   -0.10471   -0.32163    0.12525   -0.31281    0.0097207\n",
      " -0.26777   -0.61121   -0.11089   -0.13652    0.035135  -0.4939\n",
      "  0.084857  -0.15494   -0.063509  -0.23935    0.28272    0.10849\n",
      " -0.3365    -0.60764    0.38576   -0.0095438  0.17499   -0.52723\n",
      "  0.62211    0.19544   -0.48977    0.036582  -0.128     -0.016827\n",
      "  0.25647   -0.31698    0.48257   -0.14184    0.11046   -0.3098\n",
      " -0.63141   -0.37268    0.23183   -0.14268   -0.02341    0.022255\n",
      " -0.044662  -0.16404   -0.25848    0.1629     0.024751   0.23348\n",
      "  0.27933    0.38998   -0.058968   0.11355    0.15673    0.18583\n",
      " -0.19814   -0.48123   -0.035084   0.078458  -0.49833    0.10855\n",
      " -0.20133    0.05292   -0.11583   -0.16009    0.16768    0.42362\n",
      " -0.23106    0.082465   0.24296   -0.16786    0.0080409  0.085947\n",
      "  0.38033    0.072981   0.1633     0.24704   -0.11094    0.15115\n",
      " -0.22068   -0.061944  -0.037091  -0.087923  -0.23181    0.15035\n",
      " -0.19093   -0.19113   -0.11894    0.094908  -0.0043347  0.15362\n",
      " -0.41201   -0.3073     0.18375    0.40206   -0.0034793 -0.10917\n",
      " -0.69522    0.10161   -0.079256   0.40329    0.22285   -0.19374\n",
      " -0.13315    0.073231   0.099832   0.11685   -0.21643   -0.1108\n",
      "  0.10341    0.097286   0.11196   -0.3894    -0.0089363  0.28809\n",
      " -0.10792    0.028811   0.32545    0.26052   -0.038941   0.075204\n",
      "  0.46031   -0.06293    0.21661    0.17869   -0.51917    0.33591  ]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "word_embedder = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Create a test sentence\n",
    "sentence = \"hello world word embedding\"\n",
    "\n",
    "# This is how spaCy wants to embed words\n",
    "embedded_sentence = word_embedder(sentence)\n",
    "\n",
    "# Here we can see the embedding for this first word\n",
    "print(embedded_sentence[0].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204559b2",
   "metadata": {},
   "source": [
    "### Word Embeddings, how can we use them?\n",
    "\n",
    "Here we will use the distance in the embeddings space to find the most similar word in a list\n",
    "\n",
    "Feel free to play around with the word list and the example word to see how these interact with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e0290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar word to 'pen' is 'ink'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lz/dxxzxhj966zbjy685vyxqcl00000gp/T/ipykernel_77158/1267569091.py:20: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similarity = word_embedding.similarity(random_word_embeddings[i])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate a list of 100 random words\n",
    "random_words = \"acorn breeze candle drift ember falcon glint harbor ink jumble kernel latch mirth nudge orbit prism quill rumble swoop tangle umber vortex whisk yonder zeal brisk cradle dusk echo fable gloom hush ivory jigsaw knack lush minglenook oath plume quirk riddle snag throb urge verge wisp yarn zest apex blunt clasp dabble elbow flick grin hinge imprint jest knot moat nibble opal paddle quench ripple smirk trudge utter vane waddle yell zany alloy beacon clutch drape fray grasp hurl inlet jolts kink loop mural nestle ogle peep quest rove snip trill unzip vault wane yelp zipper glimmer sprout gleam twig bask clink rippled\"\n",
    "\n",
    "# Embed the random words\n",
    "random_word_embeddings = word_embedder(random_words)\n",
    "\n",
    "# Define a function to find the most similar word\n",
    "def find_most_similar_word(word, words_to_match, word_embedding_tool):\n",
    "    # Embed the input word\n",
    "    word_embedding = word_embedding_tool(word)\n",
    "\n",
    "    # Embeddings from word_list\n",
    "    random_word_embeddings = word_embedding_tool(random_words)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = []\n",
    "    for i in range(len(random_word_embeddings)):\n",
    "        similarity = word_embedding.similarity(random_word_embeddings[i])\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    # This is our most similar word\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "\n",
    "    # Return the most similar word by splitting to to match string\n",
    "    match_list = words_to_match.split(' ')\n",
    "\n",
    "    return match_list[most_similar_index] \n",
    "\n",
    "# Example usage\n",
    "input_word = \"pen\"\n",
    "most_similar_word = find_most_similar_word(input_word, random_words, word_embedder)\n",
    "print(f\"The most similar word to '{input_word}' is '{most_similar_word}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd106430",
   "metadata": {},
   "source": [
    "### A cool property about word2vec embeddings\n",
    "\n",
    "Here we are going to see how the distances in these embeddings themselves have meaning. The implication here is that there is some dimenson-like element that encodes certain properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "84154126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the kid and royal differences is: 0.47515803575515747\n",
      "The cosine similarity between the random and royal differences is: -0.0818493589758873\n",
      "The cosine similarity between the royal words is: 0.7252610921859741\n"
     ]
    }
   ],
   "source": [
    "def compute_distance(word_1, word_2, word_embedding_tool):\n",
    "    # Embed the words\n",
    "    word_1_embedding = word_embedding_tool(word_1)\n",
    "    word_2_embedding = word_embedding_tool(word_2)\n",
    "\n",
    "    # Compute the distance\n",
    "    distance = word_1_embedding.vector - word_2_embedding.vector\n",
    "    \n",
    "    return distance\n",
    "\n",
    "# Embed the royal words\n",
    "royal_difference = compute_distance(\"king\", \"queen\", word_embedder)\n",
    "\n",
    "# Embed kid words\n",
    "kid_difference = compute_distance(\"boy\", \"girl\", word_embedder)\n",
    "\n",
    "# Embed some random words\n",
    "random_difference = compute_distance(\"spaceship\", \"dog\", word_embedder)\n",
    "\n",
    "# Create difference of differences \n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Compute the cosine similarity between the two differences\n",
    "royal_kid_similarity = cosine_similarity(royal_difference, kid_difference)\n",
    "print(f\"The cosine similarity between the kid and royal differences is: {royal_kid_similarity}\")\n",
    "\n",
    "# Compute the cosine similarity between the two differences\n",
    "royal_random_similarity = cosine_similarity(royal_difference, random_difference)\n",
    "print(f\"The cosine similarity between the random and royal differences is: {royal_random_similarity}\")\n",
    "\n",
    "# Finally, show the cosine similarity between the two royal words\n",
    "royal_similarity = cosine_similarity(word_embedder(\"king\")[0].vector, word_embedder(\"queen\")[0].vector)\n",
    "print(f\"The cosine similarity between the royal words is: {royal_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af98c90d",
   "metadata": {},
   "source": [
    "## Part 2: Sentence Embeddings\n",
    "\n",
    "Now we will use small transformer models (what state of the art models like ChatGPT use!) to play with sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa1f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar sentence to 'A long journey starts with one step.' is 'A journey of a thousand miles begins with a single step.'.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# List of sentences to embed\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A journey of a thousand miles begins with a single step.\",\n",
    "    \"To be or not to be, that is the question.\",\n",
    "    \"All that glitters is not gold.\",\n",
    "    \"The pen is mightier than the sword.\"\n",
    "]\n",
    "\n",
    "# Embed the sentences\n",
    "sentence_embeddings = sentence_model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "# New example sentence\n",
    "example_sentence = \"A long journey starts with one step.\"\n",
    "\n",
    "# Embed the example sentence\n",
    "example_embedding = sentence_model.encode(example_sentence, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_similarities = util.cos_sim(example_embedding, sentence_embeddings)\n",
    "\n",
    "# Find the most similar sentence\n",
    "most_similar_idx = torch.argmax(cosine_similarities).item()\n",
    "most_similar_sentence = sentences[most_similar_idx]\n",
    "\n",
    "print(f\"The most similar sentence to '{example_sentence}' is '{most_similar_sentence}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ba1c3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Now load a much larger dataset of sentences\n",
    "ds = load_dataset(\"agentlans/high-quality-english-sentences\")\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Embed the first 10,000 sentences (this is a subset to run faster)\n",
    "sentence_embeddings = sentence_model.encode(ds['train'][:10000]['text'], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4dd0bd",
   "metadata": {},
   "source": [
    "We split out the comparison cell because the embedding of 10k sentences can take a little time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "72b7ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar sentence to 'A long journey starts with one step.' is 'Journeys are a very important part of our faith tradition, too.'.\n"
     ]
    }
   ],
   "source": [
    "# New example sentence\n",
    "example_sentence = \"A long journey starts with one step.\"\n",
    "# Embed the example sentence\n",
    "example_embedding = sentence_model.encode(example_sentence, convert_to_tensor=True)\n",
    "# Compute cosine similarities\n",
    "cosine_similarities = util.cos_sim(example_embedding, sentence_embeddings)\n",
    "# Find the most similar sentence\n",
    "most_similar_idx = torch.argmax(cosine_similarities).item()\n",
    "most_similar_sentence = ds['train'][most_similar_idx]['text']\n",
    "print(f\"The most similar sentence to '{example_sentence}' is '{most_similar_sentence}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero_to_ai_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
